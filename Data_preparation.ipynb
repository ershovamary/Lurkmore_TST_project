{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa52a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки данных с Lurkmore в csv-файл\n",
    "\n",
    "def get_page(url):\n",
    "    if not re.search('%D0%9A%D0%BE%D0%BF%D0%B8%D0%BF%D0%B0%D1%81%D1%82%D0%B0:', url):\n",
    "        req = requests.get(url)\n",
    "\n",
    "    if req.status_code == 200:\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    return None \n",
    "\n",
    "lurk_dict = {}\n",
    "lurk_data = []\n",
    "site_num = 150\n",
    "site_link = 'https://lurkmore.net/%D0%A1%D0%BB%D1%83%D0%B6%D0%B5%D0%B1%D0%BD%D0%B0%D1%8F:Random'\n",
    "\n",
    "for i in range(site_num):\n",
    "    soup = get_page(site_link)\n",
    "    s = str(soup)\n",
    "    s = s[s.find('<!-- start content -->'):s.find('<!-- end content -->') + len('<!-- end content -->')]\n",
    "    soup = BeautifulSoup(s)\n",
    "    z = soup\n",
    "    lurk_data.append(z.get_text(strip=True))\n",
    "    for sent in lurk_data:\n",
    "        tokens = sent_tokenize(sent.strip())\n",
    "        for token in tokens:\n",
    "            if re.search(r'\\s',token):\n",
    "                lurk_dict[token] = 0\n",
    "\n",
    "                \n",
    "with open('lurk_data.csv','w',encoding='utf8',newline='') as csvfile:\n",
    "    w = csv.writer(csvfile)\n",
    "    w.writerows(lurk_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0236303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки csv-файла с данными Лурка\n",
    "\n",
    "lurk_dict = pd.read_csv('lurk_data.csv')\n",
    "lurk_dict = dict(lurk_dict.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bd850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки данных из работы\n",
    "# по анализу токсичности твитов\n",
    "# и её предобработки\n",
    "\n",
    "toxic = pd.read_csv('labeled.csv')\n",
    "toxic = toxic.loc[toxic['toxic'] == 0.0]\n",
    "sentim_dict = dict(toxic.values)\n",
    "final_sent_dict = {}\n",
    "for key, value in sentim_dict.items():\n",
    "    tokens = sent_tokenize(key)\n",
    "    for token in tokens:\n",
    "        if re.search(r'\\s',token):\n",
    "            final_sent_dict[token] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a165c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки ЖЖ постов и комментов\n",
    "# из корпуса Тайга\n",
    "\n",
    "soc_dict = {}\n",
    "with open('LiveJournalPostsandcommentsGICR.txt', 'r', encoding='utf-8') as f:\n",
    "    taiga_jj = f.readlines()\n",
    "for sent in taiga_jj:\n",
    "    tokens = sent_tokenize(sent.strip())\n",
    "    for token in tokens:\n",
    "        if re.search(r'\\s',token):\n",
    "            token = re.sub(r'\\<[^>]*\\>', '', token)\n",
    "            soc_dict[token] = 0\n",
    "\n",
    "# ограничение размера корпуса\n",
    "\n",
    "small_soc_dict = {}\n",
    "for key, value in soc_dict.items():\n",
    "    if value == 0:\n",
    "        small_soc_dict[key] = value\n",
    "    if len(small_soc_dict) == 15000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6e4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки новостей Ленты\n",
    "# из корпуса Тайги\n",
    "\n",
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "dirpath = os.path.join(curr_dir, 'texts')\n",
    "fnames = []\n",
    "taiga_news = []\n",
    "news_dict = {}\n",
    "\n",
    "for root, dirs, files in os.walk(dirpath):\n",
    "    for name in files:\n",
    "        fnames.append(os.path.join(root, name))\n",
    "\n",
    "for fpath in fnames:\n",
    "    with open(fpath, 'r', encoding='utf-8') as f:\n",
    "        text = ''.join([str(item) for item in f.readlines()])\n",
    "        taiga_news.append(text)\n",
    "for sent in taiga_news:\n",
    "    tokens = sent_tokenize(sent)\n",
    "    for token in tokens:\n",
    "        if re.search(r'\\s',token):\n",
    "            news_dict[token.strip()] = 1\n",
    "\n",
    "# ограничение размера корпуса\n",
    "\n",
    "small_news_dict = {}\n",
    "for key, value in news_dict.items():\n",
    "    if value == 1:\n",
    "        small_news_dict[key] = value\n",
    "    if len(small_news_dict) == 49000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5cdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки данных из Википедии\n",
    "\n",
    "wiki_dict = {}\n",
    "with open('wiki_data.txt', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        tokens = sent_tokenize(line)\n",
    "        for token in tokens:\n",
    "            if re.search(r'\\.',token) and re.search(r'\\s',token):\n",
    "                wiki_dict[token.strip()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da6b974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки отобранных данных из Лурка\n",
    "# с их формальными аналогами\n",
    "\n",
    "lurk_test = pd.read_csv('lurk_test.csv')\n",
    "lurk_test_dict = dict(lurk_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ee7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для создания тестовой выборки\n",
    "\n",
    "def get_test(d, n, is_formal):\n",
    "    test_dict = {}\n",
    "    for k in random.sample(list(d), n):\n",
    "        test_dict[k] = is_formal\n",
    "        del d[k]\n",
    "    return test_dict\n",
    "\n",
    "test_lurk_dict = get_test(lurk_dict, 100, 0)\n",
    "test_dict_sent = get_test(final_sent_dict, 100, 0)\n",
    "test_dict_soc = get_test(small_soc_dict, 100, 0)\n",
    "test_dict_news = get_test(small_news_dict, 100, 1)\n",
    "test_dict_wiki = get_test(wiki_dict, 100, 1)\n",
    "\n",
    "\n",
    "test_dict = {**lurk_test_dict, **test_dict_sent}\n",
    "test_dict = {**test_dict, **test_dict_soc}\n",
    "test_dict = {**test_dict, **test_dict_news}\n",
    "test_dict = {**test_dict, **test_dict_wiki}\n",
    "test_dict = {**test_dict, **test_lurk_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a4d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для загрузки чистой тестовой выборки\n",
    "# из csv-файла размером 500 предложений\n",
    "\n",
    "test_dict = pd.read_csv('cleared_test_data.csv')\n",
    "test_dict = dict(test_dict.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc0f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ячейка для создания единого трэйна\n",
    "\n",
    "train_dict = {**final_sent_dict, **small_soc_dict}\n",
    "train_dict = {**train_dict, **small_news_dict}\n",
    "train_dict = {**train_dict, **wiki_dict}\n",
    "train_dict = {**train_dict, **lurk_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "706b69e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef51fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка данных для векторизации с помощью sklearn\n",
    "\n",
    "data_train = list(train_dict.keys())\n",
    "data_test = list(test_dict.keys())\n",
    "\n",
    "train_target = list(train_dict.values())\n",
    "test_target = list(test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c87392bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизация данных с помощью CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(data_train)\n",
    "X_test_counts = count_vect.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d41125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       318\n",
      "           1       0.92      0.87      0.90       223\n",
      "\n",
      "    accuracy                           0.92       541\n",
      "   macro avg       0.92      0.91      0.92       541\n",
      "weighted avg       0.92      0.92      0.92       541\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vfif\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# проверка качества классификатора с помощью функции логистической регрессии\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_counts, train_target)\n",
    "pred = clf.predict(X_test_counts)\n",
    "print(classification_report(test_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5104292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       318\n",
      "           1       0.92      0.87      0.90       223\n",
      "\n",
      "    accuracy                           0.92       541\n",
      "   macro avg       0.92      0.91      0.91       541\n",
      "weighted avg       0.92      0.92      0.92       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверка качества классификатора с помощью метода опорных векторов\n",
    "\n",
    "clf = LinearSVC().fit(X_train_counts, train_target)\n",
    "pred = clf.predict(X_test_counts)\n",
    "print(classification_report(test_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66c0ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизация данных с помощью TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "X_train_counts = tfidf_vect.fit_transform(data_train)\n",
    "X_test_counts = tfidf_vect.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faa99aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       318\n",
      "           1       0.90      0.80      0.85       223\n",
      "\n",
      "    accuracy                           0.88       541\n",
      "   macro avg       0.88      0.87      0.87       541\n",
      "weighted avg       0.88      0.88      0.88       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверка качества классификатора с помощью функции логистической регрессии\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_counts, train_target)\n",
    "pred = clf.predict(X_test_counts)\n",
    "print(classification_report(test_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d69a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       318\n",
      "           1       0.93      0.90      0.91       223\n",
      "\n",
      "    accuracy                           0.93       541\n",
      "   macro avg       0.93      0.92      0.93       541\n",
      "weighted avg       0.93      0.93      0.93       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверка качества классификатора с помощью метода опорных векторов\n",
    "\n",
    "clf = LinearSVC().fit(X_train_counts, train_target)\n",
    "pred = clf.predict(X_test_counts)\n",
    "print(classification_report(test_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0564d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
